{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected binary or unicode string, got [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m (train_texts, train_labels), (test_texts, test_labels) \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mimdb\u001b[39m.\u001b[39mload_data()\n\u001b[0;32m      8\u001b[0m \u001b[39m# Convert the integer-encoded texts to strings\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m train_texts \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39;49mcompat\u001b[39m.\u001b[39;49mas_str(text) \u001b[39mfor\u001b[39;49;00m text \u001b[39min\u001b[39;49;00m train_texts]\n\u001b[0;32m     10\u001b[0m test_texts \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mas_str(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m test_texts]\n\u001b[0;32m     12\u001b[0m \u001b[39m# Split the dataset into train and validation sets\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m (train_texts, train_labels), (test_texts, test_labels) \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mimdb\u001b[39m.\u001b[39mload_data()\n\u001b[0;32m      8\u001b[0m \u001b[39m# Convert the integer-encoded texts to strings\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m train_texts \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39;49mcompat\u001b[39m.\u001b[39;49mas_str(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m train_texts]\n\u001b[0;32m     10\u001b[0m test_texts \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mas_str(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m test_texts]\n\u001b[0;32m     12\u001b[0m \u001b[39m# Split the dataset into train and validation sets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ARUNDHATI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\compat.py:116\u001b[0m, in \u001b[0;36mas_str\u001b[1;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mas_str\u001b[39m(bytes_or_text, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 116\u001b[0m   \u001b[39mreturn\u001b[39;00m as_text(bytes_or_text, encoding)\n",
      "File \u001b[1;32mc:\\Users\\ARUNDHATI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\compat.py:112\u001b[0m, in \u001b[0;36mas_text\u001b[1;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[0;32m    110\u001b[0m   \u001b[39mreturn\u001b[39;00m bytes_or_text\u001b[39m.\u001b[39mdecode(encoding)\n\u001b[0;32m    111\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 112\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mExpected binary or unicode string, got \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m bytes_or_text)\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected binary or unicode string, got [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the IMDB Movie Reviews dataset\n",
    "(train_texts, train_labels), (test_texts, test_labels) = tf.keras.datasets.imdb.load_data()\n",
    "\n",
    "# Convert the integer-encoded texts to strings\n",
    "train_texts = [tf.compat.as_str(text) for text in train_texts]\n",
    "test_texts = [tf.compat.as_str(text) for text in test_texts]\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode the text data\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    test_labels\n",
    "))\n",
    "\n",
    "# Define the BERT model for text classification\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset.shuffle(1000).batch(16),\n",
    "          epochs=5,\n",
    "          batch_size=16,\n",
    "          validation_data=val_dataset.batch(16))\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset.batch(16))\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
